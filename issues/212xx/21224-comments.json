[
   {
      "author_association" : "CONTRIBUTOR",
      "body" : "> This PR halts the processing of unrequested transactions in Bitcoin at TX message reception. An unrequested transaction is one defined by which a \"getdata\" message for its specific identifier (either txid or wtxid) has not been previously issued by the node.\r\n\r\nI don't see the point of this? An attacker with valid utxos can generate invalid txs that are costly to reject, and simply announce their txids and wait for a request. That's limited to 5000 txs in a batch (MAX_PEER_TX_ANNOUNCEMENTS), and I think each batch would have at least a 4s window (NONPREF_PEER_TX_DELAY + OVERLOADED_PEER_TX_DELAY). But that seems like it should be enough to keep the CPU busy? Even then, we'll make progress through every other node in between dealing with each tx from the attacker, so this shouldn't cause a denial of service as far as I can see? Presuming the tx's are consensus invalid, the attacker will get given a Misbehaving score for each invalid tx and eventually kicked.\r\n\r\n> It comes with a meaningful delay of running the framework (e.g +4min to run p2p_segwit.py)\r\n\r\nErr nack? :) Maybe better to add PF_RELAY to those tests than slow them down?",
      "created_at" : "2021-02-18T14:48:49Z",
      "html_url" : "https://github.com/bitcoin/bitcoin/pull/21224#issuecomment-781395303",
      "id" : 781395303,
      "issue_url" : "https://api.github.com/repos/bitcoin/bitcoin/issues/21224",
      "node_id" : "MDEyOklzc3VlQ29tbWVudDc4MTM5NTMwMw==",
      "performed_via_github_app" : null,
      "updated_at" : "2021-02-18T14:48:49Z",
      "url" : "https://api.github.com/repos/bitcoin/bitcoin/issues/comments/781395303",
      "user" : {
         "avatar_url" : "https://avatars.githubusercontent.com/u/127186?v=4",
         "events_url" : "https://api.github.com/users/ajtowns/events{/privacy}",
         "followers_url" : "https://api.github.com/users/ajtowns/followers",
         "following_url" : "https://api.github.com/users/ajtowns/following{/other_user}",
         "gists_url" : "https://api.github.com/users/ajtowns/gists{/gist_id}",
         "gravatar_id" : "",
         "html_url" : "https://github.com/ajtowns",
         "id" : 127186,
         "login" : "ajtowns",
         "node_id" : "MDQ6VXNlcjEyNzE4Ng==",
         "organizations_url" : "https://api.github.com/users/ajtowns/orgs",
         "received_events_url" : "https://api.github.com/users/ajtowns/received_events",
         "repos_url" : "https://api.github.com/users/ajtowns/repos",
         "site_admin" : false,
         "starred_url" : "https://api.github.com/users/ajtowns/starred{/owner}{/repo}",
         "subscriptions_url" : "https://api.github.com/users/ajtowns/subscriptions",
         "type" : "User",
         "url" : "https://api.github.com/users/ajtowns"
      }
   },
   {
      "author_association" : "CONTRIBUTOR",
      "body" : "<!--e57a25ab6845829454e8d69fc972939a-->\n\nThe following sections might be updated with supplementary metadata relevant to reviewers and maintainers.\n\n<!--174a7506f384e20aa4161008e828411d-->\n### Conflicts\nReviewers, this pull request conflicts with the following ones:\n\n* #21160 (Net/Net processing: Move tx inventory into net_processing by jnewbery)\n* #21148 (Split orphan handling from net_processing into txorphanage by ajtowns)\n\nIf you consider this pull request important, please also help to review the conflicting pull requests. Ideally, start with the one that should be merged first.",
      "created_at" : "2021-02-18T14:57:32Z",
      "html_url" : "https://github.com/bitcoin/bitcoin/pull/21224#issuecomment-781401545",
      "id" : 781401545,
      "issue_url" : "https://api.github.com/repos/bitcoin/bitcoin/issues/21224",
      "node_id" : "MDEyOklzc3VlQ29tbWVudDc4MTQwMTU0NQ==",
      "performed_via_github_app" : null,
      "reactions" : {
         "+1" : 0,
         "-1" : 0,
         "confused" : 0,
         "eyes" : 0,
         "heart" : 0,
         "hooray" : 0,
         "laugh" : 0,
         "rocket" : 0,
         "total_count" : 0,
         "url" : "https://api.github.com/repos/bitcoin/bitcoin/issues/comments/781401545/reactions"
      },
      "updated_at" : "2021-02-25T19:47:00Z",
      "url" : "https://api.github.com/repos/bitcoin/bitcoin/issues/comments/781401545",
      "user" : {
         "avatar_url" : "https://avatars.githubusercontent.com/u/39886733?v=4",
         "events_url" : "https://api.github.com/users/DrahtBot/events{/privacy}",
         "followers_url" : "https://api.github.com/users/DrahtBot/followers",
         "following_url" : "https://api.github.com/users/DrahtBot/following{/other_user}",
         "gists_url" : "https://api.github.com/users/DrahtBot/gists{/gist_id}",
         "gravatar_id" : "",
         "html_url" : "https://github.com/DrahtBot",
         "id" : 39886733,
         "login" : "DrahtBot",
         "node_id" : "MDQ6VXNlcjM5ODg2NzMz",
         "organizations_url" : "https://api.github.com/users/DrahtBot/orgs",
         "received_events_url" : "https://api.github.com/users/DrahtBot/received_events",
         "repos_url" : "https://api.github.com/users/DrahtBot/repos",
         "site_admin" : false,
         "starred_url" : "https://api.github.com/users/DrahtBot/starred{/owner}{/repo}",
         "subscriptions_url" : "https://api.github.com/users/DrahtBot/subscriptions",
         "type" : "User",
         "url" : "https://api.github.com/users/DrahtBot"
      }
   },
   {
      "author_association" : "MEMBER",
      "body" : "@ajtowns \r\n\r\n>  I don't see the point of this? An attacker with valid utxos can generate invalid txs that are costly to reject, and simply announce their txids and wait for a request. That's limited to 5000 txs in a batc>h (    MAX_PEER_TX_ANNOUNCEMENTS), and I think each batch would have at least a 4s window (NONPREF_PEER_TX_DELAY + OVERLOADED_PEER_TX_DELAY). \r\n\r\nAre you saying this change isn't worhty because it doesn't prevent further tx-relay DoS ? As of today, an attacker doesn't have to \"wait\" for a request and can just spam us we costly-to-evaluate transactions. Rate-limiting DoSy peers beyond `MAX_TX_PEER_ANNOUNCEMENTS` is already a strict improvement, you force a time burden on them.\r\n\r\n> But that seems like it should be enough to keep the CPU busy?\r\n\r\nOnce you're dropping the unrequested transactions, you might start to squeeze a peer announcement buffer if its top-of-transaction-to-evaluate buffer is too slow/costly to evaluate. But that kind of further mitigations is useless if you can bypass it with unrequested transactions.\r\n\r\n> Even then, we'll make progress through every other node in between dealing with each tx from the attacker, so this shouldn't cause a denial of service as far as I can see?\r\n\r\nIf you have % of inbound peers being malicious and DoSing you, this starts to be a concern. What's the % is a more serious question.\r\n\r\n> Presuming the tx's are consensus invalid, the attacker will get given a Misbehaving score for each invalid tx > and eventually kicked.\r\n\r\nIf you're a motivated attacker, this is super easy to avoid the kick-out. Just craft malicious non-standard junk (e.g make the sig of the last input high-s) and you will avoid `MaybePunishNodeForTx` ban. Generally, we should assume an attacker to be always able to circumvent bans triggered from invalid transactions, just target discrepancies in tx-relay policies.\r\n\r\nNote, this mitigation proposal was a suggestion from @sdaftuar, following some CPU grinding of its own.\r\n\r\n> The last time I looked at this, it seemed like spending on the order of 1 second (on my modern workstation) to validate a transaction that would ultimately fail mempool acceptance was easily achievable; multiply that by the number of inbound connections we allow and you get a pretty bad state of affairs...\r\n\r\n(in #20277)\r\n\r\nThat said, I concede it would be better to have a common attacker model (number of malicious inbound, worst-case transactions,  CPU resources on a middle-grade host, I/O layout of UTXOs fetched, ...) before to work or propose eventual tx-relay DoS mitigations. If so, I would be happy to do such investigations.\r\n\r\n> Err nack? :) Maybe better to add PF_RELAY to those tests than slow them down?\r\n\r\nYeah I feel the same. I fixed the tests just-in-case someone was willingly to analyze tests impacted, at least I've a branch...",
      "created_at" : "2021-02-18T18:18:46Z",
      "html_url" : "https://github.com/bitcoin/bitcoin/pull/21224#issuecomment-781541708",
      "id" : 781541708,
      "issue_url" : "https://api.github.com/repos/bitcoin/bitcoin/issues/21224",
      "node_id" : "MDEyOklzc3VlQ29tbWVudDc4MTU0MTcwOA==",
      "performed_via_github_app" : null,
      "updated_at" : "2021-02-18T18:21:01Z",
      "url" : "https://api.github.com/repos/bitcoin/bitcoin/issues/comments/781541708",
      "user" : {
         "avatar_url" : "https://avatars.githubusercontent.com/u/23310655?v=4",
         "events_url" : "https://api.github.com/users/ariard/events{/privacy}",
         "followers_url" : "https://api.github.com/users/ariard/followers",
         "following_url" : "https://api.github.com/users/ariard/following{/other_user}",
         "gists_url" : "https://api.github.com/users/ariard/gists{/gist_id}",
         "gravatar_id" : "",
         "html_url" : "https://github.com/ariard",
         "id" : 23310655,
         "login" : "ariard",
         "node_id" : "MDQ6VXNlcjIzMzEwNjU1",
         "organizations_url" : "https://api.github.com/users/ariard/orgs",
         "received_events_url" : "https://api.github.com/users/ariard/received_events",
         "repos_url" : "https://api.github.com/users/ariard/repos",
         "site_admin" : false,
         "starred_url" : "https://api.github.com/users/ariard/starred{/owner}{/repo}",
         "subscriptions_url" : "https://api.github.com/users/ariard/subscriptions",
         "type" : "User",
         "url" : "https://api.github.com/users/ariard"
      }
   },
   {
      "author_association" : "MEMBER",
      "body" : "@ajtowns I think not processing unrequested transactions is a first step towards being able to protect the node from transaction-based CPU DoS; it's of course not helpful on its own.  I think as a next step we could consider keeping some sort of score on peers based on how many slow-to-validate transactions they've relayed to us that were not accepted to the mempool, and for badly scoring peers (or new peers that have just connected to us) we could have a longer delay between inv and getdata, or tolerate fewer transactions in flight (or both).\r\n\r\nI haven't tried to figure out exactly what these measures would look like in code or exactly how much benefit we would get, so for sure this is speculative, but I also don't think any of those ideas work at all if we don't first stop processing unrequested transactions -- which seems like both a harmless and intuitive way for relay to work.\r\n",
      "created_at" : "2021-02-18T19:01:38Z",
      "html_url" : "https://github.com/bitcoin/bitcoin/pull/21224#issuecomment-781567152",
      "id" : 781567152,
      "issue_url" : "https://api.github.com/repos/bitcoin/bitcoin/issues/21224",
      "node_id" : "MDEyOklzc3VlQ29tbWVudDc4MTU2NzE1Mg==",
      "performed_via_github_app" : null,
      "updated_at" : "2021-02-18T19:01:38Z",
      "url" : "https://api.github.com/repos/bitcoin/bitcoin/issues/comments/781567152",
      "user" : {
         "avatar_url" : "https://avatars.githubusercontent.com/u/7463573?v=4",
         "events_url" : "https://api.github.com/users/sdaftuar/events{/privacy}",
         "followers_url" : "https://api.github.com/users/sdaftuar/followers",
         "following_url" : "https://api.github.com/users/sdaftuar/following{/other_user}",
         "gists_url" : "https://api.github.com/users/sdaftuar/gists{/gist_id}",
         "gravatar_id" : "",
         "html_url" : "https://github.com/sdaftuar",
         "id" : 7463573,
         "login" : "sdaftuar",
         "node_id" : "MDQ6VXNlcjc0NjM1NzM=",
         "organizations_url" : "https://api.github.com/users/sdaftuar/orgs",
         "received_events_url" : "https://api.github.com/users/sdaftuar/received_events",
         "repos_url" : "https://api.github.com/users/sdaftuar/repos",
         "site_admin" : false,
         "starred_url" : "https://api.github.com/users/sdaftuar/starred{/owner}{/repo}",
         "subscriptions_url" : "https://api.github.com/users/sdaftuar/subscriptions",
         "type" : "User",
         "url" : "https://api.github.com/users/sdaftuar"
      }
   },
   {
      "author_association" : "CONTRIBUTOR",
      "body" : "If a single tx can take 1s to validate and fail for policy reasons to avoid getting disconnected/banned, then a single peer can generate a backlog of about 1h20m (5000 seconds) of processing, beginning only 2s after getting connected using INV/GETDATA rather than just using TX, so this doesn't seem like a big help?\r\n\r\nI think an attacker could also queue ~50 slow orphan txs, have them all downloaded without revealing anything slow is happening, and then activate them by relaying a good parent for them -- without INV/GETDATA delays only able to prevent them from doing the next round of 50 slows txs. Depending on the scoring mechanism, rotating that sort of spam amongst a few peers might be enough to keep 100% load without much hassle.\r\n\r\nThe way I've been looking at this is that INV/GETDATA (and eventually erlay) is just a way to avoid wasting bandwidth, and that we should be robust to having to validate any random tx that anyone decides to send to us. Avoiding wasting bandwidth only matters for honest peers not attackers, who can presumably find plenty of ways to send us garbage data -- masses of PINGs eg. That seems easier to design for to me -- in particular, I think it might be hard to get txrequest to handle changes in the delay between INV/GETDATA efficiently, which might mean adding DoS vectors rather than removing them.\r\n\r\nAs an alternative approach, if we think a peer is wasting too much time, we could avoid calling `ProcessMessages()` for it at all until they're back at a reasonable level, relying on hitting the `fPauseRecv=true` flag as new messages get queued, and it not being cleared again until we start doing `ProcessMessages()` again. We'd need to ensure that orphan tx's get put in the from peer's workset instead of the workset of whichever peer relayed the parent that made the tx minable, but that seems fine. Something like:\r\n\r\n```c++\r\nstd::chrono::microseconds CNode::delay_processing_until{0};\r\n\r\nfor (pnode : vNodes) {\r\n    // for caling ProcessMessages\r\n    start = GetTimeMicros();\r\n    if (start > pnode->delay_processing_until) {\r\n        ProcessMessages(pnode);\r\n        fin = GetTimeMicros();\r\n        pnode->delay_processing_until = std::clamp(pnode->delay_processing_until + 20*(fin-start), fin - 10s, fin + 60s);\r\n    }\r\n}\r\n```\r\n\r\n(I think the factor of 20 there would means each peer can only use up 5% of your CPU)\r\n\r\nEDIT: Changed to use `std::clamp` so that peers don't get delayed too excessively. I've tried this out now (along with some changes to make it monitorable) on a node with no inbound connections; it takes about 5 minutes for the mempool to load at present and peers get delayed while that's happening, but otherwise it seems to have no effect, as expected in the absence of an attack.",
      "created_at" : "2021-02-19T01:05:38Z",
      "html_url" : "https://github.com/bitcoin/bitcoin/pull/21224#issuecomment-781741925",
      "id" : 781741925,
      "issue_url" : "https://api.github.com/repos/bitcoin/bitcoin/issues/21224",
      "node_id" : "MDEyOklzc3VlQ29tbWVudDc4MTc0MTkyNQ==",
      "performed_via_github_app" : null,
      "updated_at" : "2021-02-19T05:17:04Z",
      "url" : "https://api.github.com/repos/bitcoin/bitcoin/issues/comments/781741925",
      "user" : {
         "avatar_url" : "https://avatars.githubusercontent.com/u/127186?v=4",
         "events_url" : "https://api.github.com/users/ajtowns/events{/privacy}",
         "followers_url" : "https://api.github.com/users/ajtowns/followers",
         "following_url" : "https://api.github.com/users/ajtowns/following{/other_user}",
         "gists_url" : "https://api.github.com/users/ajtowns/gists{/gist_id}",
         "gravatar_id" : "",
         "html_url" : "https://github.com/ajtowns",
         "id" : 127186,
         "login" : "ajtowns",
         "node_id" : "MDQ6VXNlcjEyNzE4Ng==",
         "organizations_url" : "https://api.github.com/users/ajtowns/orgs",
         "received_events_url" : "https://api.github.com/users/ajtowns/received_events",
         "repos_url" : "https://api.github.com/users/ajtowns/repos",
         "site_admin" : false,
         "starred_url" : "https://api.github.com/users/ajtowns/starred{/owner}{/repo}",
         "subscriptions_url" : "https://api.github.com/users/ajtowns/subscriptions",
         "type" : "User",
         "url" : "https://api.github.com/users/ajtowns"
      }
   },
   {
      "author_association" : "MEMBER",
      "body" : "> If a single tx can take 1s to validate and fail for policy reasons to avoid getting disconnected/banned, then a single peer can generate a backlog of about 1h20m (5000 seconds) of processing, beginning only 2s after getting connected using INV/GETDATA rather than just using TX, so this doesn't seem like a big help?\r\n\r\nThat assumes that we don't limit the number of transactions in flight to each peer, which I think we could do as well.  I think it'd be reasonable to limit to something like 5 in-flight transactions to a new peer (say) until we have some track record of accepting the transactions they send us, and then increasing it.  \r\n \r\n> I think an attacker could also queue ~50 slow orphan txs, have them all downloaded without revealing anything slow is happening, and then activate them by relaying a good parent for them -- without INV/GETDATA delays only able to prevent them from doing the next round of 50 slows txs. Depending on the scoring mechanism, rotating that sort of spam amongst a few peers might be enough to keep 100% load without much hassle.\r\n\r\nI hadn't thought of using orphans in this way -- good point!  I'll have to give this more thought, but presumably a good first start would just be to ensure orphan validation times get accounted to the peers that relay them, and then maybe the number of in-flight orphans from a peer should be limited?  In theory, if we made a change so that we generally only request missing ancestors from the peer that relays an orphan (via a new p2p message to request all of them at once, a la package relay), then I think that should make the accounting easier (since we'd never be commingling one peer's outstanding transactions with another), but this probably would cause some multiplicative factor in the number of in-flight transactions to a peer.\r\n\r\n> The way I've been looking at this is that INV/GETDATA (and eventually erlay) is just a way to avoid wasting bandwidth, and that we should be robust to having to validate any random tx that anyone decides to send to us. Avoiding wasting bandwidth only matters for honest peers not attackers, who can presumably find plenty of ways to send us garbage data -- masses of PINGs eg. That seems easier to design for to me -- in particular, I think it might be hard to get txrequest to handle changes in the delay between INV/GETDATA efficiently, which might mean adding DoS vectors rather than removing them.\r\n\r\nI have not given any thought to whether the things I've said would be easy or hard to implement in the new txrequest class.  So you may be right that these ideas would take a lot of work...  However I do think that your proposed strategy of delaying all messages from a peer has problems.  In particular it means that honest peers relaying us a lot of valid transactions, and maybe also blocks, would have all their messages delayed relative to less useful peers -- which could easily be expected to slow down block propagation.  (Also it seems like as you coded it, there could be plenty of times when we don't process any peers' traffic, because they all are in a suspended state, which just seems wasteful and will increase our backlogs and latency.)\r\n\r\nI think another problem with this strategy is that once you establish 21 inbound connections to a peer running with this logic, then they can work together to grant each other more CPU time and completely bypass the delays -- the first peer uses 0.25 seconds of CPU and is stalled for (say) 5 seconds.  The next 20 peers that get to run in the same loop iteration do the same, thing, so that by the time the first peer is up again, it's time for it to run.  So an attacker who gets to 21 connections is no longer limited by the logic.  While increasing that 20 number would make the attack harder for an adversary to carry out, it also has a tradeoff of delaying processing of honest peer's traffic too, so I'm not sure how to tune it in a way that would both be protective and not add undesirable latency.\r\n\r\n--\r\n\r\nI think the fundamental problem here is that we will have lots of cases where a peer can be getting us to use our CPU and it will not be possible for us to tell if the peer is adversarial or not.  That's basically the issue with transactions that fail policy checks but pass consensus checks: we can try to be more careful than we currently are about maybe classifying some of those policy checks (such as now-old soft forks, like DERSIG) as consensus rather than policy, but at the end of the day there will always be cases that we aren't sure about.  For instance, when we loosen a policy requirement, then older software will think that newer software is relaying invalid stuff, and it's hard for a single node to tell that the looser policy is the one that maybe all its peers are using.  (And obviously in general we should be tolerant of different policies on the network, whether due to user modifications, different software, etc.)\r\n\r\nI guess the other problem (which I think your patch runs into) is that we want to always be using our CPU to process something if we have data to read, but in a way that guarantees that no peer or group of peers can cause us to not service the connection of a different peer.\r\n\r\n--\r\n\r\nGetting back to this proposal: what do you think the downside is of adopting this change, and not processing unrequested transactions?  If it breaks a lot of existing software, I think that would be an argument for giving people time to fix their stuff and not merging it in a hurry.  If there are old wallets that rely on this behavior (to random peers, who aren't going to give them PF_RELAY) then that would probably be a good argument to shelve this altogether I guess, and try to find another way[*] .  Do we think there is such software out there? \r\n\r\nOtherwise, I think if we establish that transaction relay on the network should work via INV and then GETDATA, then that just gives us more freedom to design anti-DoS strategies than if we have a mindset of being required to accept unrequested transactions.  My impression is that there's no clear consensus that unrequested transactions should be permitted, so I thought that making that clearer by sending a note to the mailing list, and see if there are any responses, might solidify that understanding with the developer community.  Maybe it's enough to stop there and delay deploying changes to our code until we have something more fleshed out, but this all started because @ariard wanted to stop processing unrequested transactions during IBD -- and I figure that we might as well just do this in general, if that is our understanding, if we're going to do a change like that at all.\r\n\r\n\r\n\r\n[*] For example, I proposed #15169 as a mitigation; at some point now that wtxidrelay has started to be deployed, I hope that we will eventually get rid of 2 of the 3 calls to `CheckInputs` too, since eventually `net_processing` won't need to care about whether failure is due to being WITNESS_STRIPPED.  I believe the combined effect of these two changes would be substantial, so maybe we might decide to not worry about this further if we did both of these things.",
      "created_at" : "2021-02-19T14:17:03Z",
      "html_url" : "https://github.com/bitcoin/bitcoin/pull/21224#issuecomment-782102604",
      "id" : 782102604,
      "issue_url" : "https://api.github.com/repos/bitcoin/bitcoin/issues/21224",
      "node_id" : "MDEyOklzc3VlQ29tbWVudDc4MjEwMjYwNA==",
      "performed_via_github_app" : null,
      "updated_at" : "2021-02-19T14:17:03Z",
      "url" : "https://api.github.com/repos/bitcoin/bitcoin/issues/comments/782102604",
      "user" : {
         "avatar_url" : "https://avatars.githubusercontent.com/u/7463573?v=4",
         "events_url" : "https://api.github.com/users/sdaftuar/events{/privacy}",
         "followers_url" : "https://api.github.com/users/sdaftuar/followers",
         "following_url" : "https://api.github.com/users/sdaftuar/following{/other_user}",
         "gists_url" : "https://api.github.com/users/sdaftuar/gists{/gist_id}",
         "gravatar_id" : "",
         "html_url" : "https://github.com/sdaftuar",
         "id" : 7463573,
         "login" : "sdaftuar",
         "node_id" : "MDQ6VXNlcjc0NjM1NzM=",
         "organizations_url" : "https://api.github.com/users/sdaftuar/orgs",
         "received_events_url" : "https://api.github.com/users/sdaftuar/received_events",
         "repos_url" : "https://api.github.com/users/sdaftuar/repos",
         "site_admin" : false,
         "starred_url" : "https://api.github.com/users/sdaftuar/starred{/owner}{/repo}",
         "subscriptions_url" : "https://api.github.com/users/sdaftuar/subscriptions",
         "type" : "User",
         "url" : "https://api.github.com/users/sdaftuar"
      }
   },
   {
      "author_association" : "CONTRIBUTOR",
      "body" : "Rearranging:\r\n\r\n> Getting back to this proposal: what do you think the downside is of adopting this change, and not processing unrequested transactions? If it breaks a lot of existing software, [...]\r\n\r\nI don't think it solves the problem -- `INV/GETDATA` needs to be able to feed us ~7 tx/s to us just to keep up with block capacity and RBF, and if we receive 7 tx's that take 1s to validate each second, we already can't keep up.\r\n\r\nI just don't see the `INV/GETDATA` pairing as adding any useful information beyond de-duping across peers; if what we want to do is rate-limit `TX` messages we receive from a peer IMO we should literally do that -- and we can do it when we get up to the `TX` message by pausing processing for that peer, there's no need to complicate it by adding it into the INV handling.\r\n\r\n> > If a single tx can take 1s to validate and fail for policy reasons to avoid getting disconnected/banned, then a single peer can generate a backlog of about 1h20m (5000 seconds) of processing, beginning only 2s after getting connected using INV/GETDATA rather than just using TX, so this doesn't seem like a big help? \r\n> That assumes that we don't limit the number of transactions in flight to each peer, which I think we could do as well.\r\n\r\nWe dropped per-peer inflight limits with txrequest; see the rationale in the PR summary for #19988. The argument was similar to your point about my patch potentially meaning we just sit idle because we're punishing all peers -- we don't want to limit inflights from any peer because that might delay us hearing about a transaction only offered by that peer.\r\n\r\n>  I think it'd be reasonable to limit to something like 5 in-flight transactions to a new peer (say) until we have some track record of accepting the transactions they send us, and then increasing it.\r\n\r\nWouldn't we expect an attacker to just wait until we've increased the limit before attacking us?\r\n \r\n> I hadn't thought of using orphans in this way -- good point! I'll have to give this more thought, but presumably a good first start would just be to ensure orphan validation times get accounted to the peers that relay them,\r\n\r\nYes, I think so. I think after #21148 we could move the orphan worksets out of the per-peer structures and into txorphanage directly which I think would make this a little easier.\r\n\r\nHmm, actually I don't think even a per-peer-orphan limit would help enough? If you're attack is just increasing latency, then you could connect 20 peers to your victim, have each propose 5 orphan txs that will all take 1s to fail to validate and all depend on a single valid tx; then relay the valid tx. On the next run, you'll take take 20s to process the first of each of those orphans; the run after that you'll take 20s to process the next set, etc -- but your honest peer will also only be getting one message processed in each of these sets of 20s too....\r\n \r\n> However I do think that your proposed strategy of delaying all messages from a peer has problems. In particular it means that honest peers relaying us a lot of valid transactions, and maybe also blocks, would have all their messages delayed relative to less useful peers -- which could easily be expected to slow down block propagation. (Also it seems like as you coded it, there could be plenty of times when we don't process any peers' traffic, because they all are in a suspended state, which just seems wasteful and will increase our backlogs and latency.)\r\n\r\nPeers that don't do much work will end up with a 10s buffer (`delay_until = fin-10s`) so they'd need to actually use up ~an entire~ half a second in ProcessMessages before they'll get delayed at all -- I'm seeing something like 100 potential calls to ProcessMessages being missed per peer (so a total of 10s of additional latency -- though I'm not checking whether there's any actual work for the peer to do so it might just be latency for ProcessMessages saying \"nope, nothing to do\") for peers that have been connected for about 10 hours.\r\n\r\nIt was the simplest token-bucket like thing I could think of in five minutes, so that's already performing way better than I expected...\r\n\r\n> I think the fundamental problem here is that we will have lots of cases where a peer can be getting us to use our CPU and it will not be possible for us to tell if the peer is adversarial or not. That's basically the issue with transactions that fail policy checks but pass consensus checks: [...]\r\n\r\nSo... the next thought that comes to mind is what if `ProcessMessages` returned a \"that message was useful!\" boolean, and we only considered it a waste of time when the boolean indicated it was useless? So non-standard, too-low-fee, too-many-in-mempool-ancestors, pong-that-wasn't-in-response-to-a-ping, etc? That is, have a per-peer token bucket of some sort for useless messages, and temporarily de-prioritise peers for useless messages, without building up a misbehaving/ban score. If a peer is sending hard-to-validate transactions that are actually valid and make it into our mempool, we'll forward them, and therefore we can't consider the peer to be an attacker, even if this is an attack, I think?",
      "created_at" : "2021-02-19T16:18:31Z",
      "html_url" : "https://github.com/bitcoin/bitcoin/pull/21224#issuecomment-782178622",
      "id" : 782178622,
      "issue_url" : "https://api.github.com/repos/bitcoin/bitcoin/issues/21224",
      "node_id" : "MDEyOklzc3VlQ29tbWVudDc4MjE3ODYyMg==",
      "performed_via_github_app" : null,
      "updated_at" : "2021-02-19T16:20:08Z",
      "url" : "https://api.github.com/repos/bitcoin/bitcoin/issues/comments/782178622",
      "user" : {
         "avatar_url" : "https://avatars.githubusercontent.com/u/127186?v=4",
         "events_url" : "https://api.github.com/users/ajtowns/events{/privacy}",
         "followers_url" : "https://api.github.com/users/ajtowns/followers",
         "following_url" : "https://api.github.com/users/ajtowns/following{/other_user}",
         "gists_url" : "https://api.github.com/users/ajtowns/gists{/gist_id}",
         "gravatar_id" : "",
         "html_url" : "https://github.com/ajtowns",
         "id" : 127186,
         "login" : "ajtowns",
         "node_id" : "MDQ6VXNlcjEyNzE4Ng==",
         "organizations_url" : "https://api.github.com/users/ajtowns/orgs",
         "received_events_url" : "https://api.github.com/users/ajtowns/received_events",
         "repos_url" : "https://api.github.com/users/ajtowns/repos",
         "site_admin" : false,
         "starred_url" : "https://api.github.com/users/ajtowns/starred{/owner}{/repo}",
         "subscriptions_url" : "https://api.github.com/users/ajtowns/subscriptions",
         "type" : "User",
         "url" : "https://api.github.com/users/ajtowns"
      }
   },
   {
      "author_association" : "MEMBER",
      "body" : "@ajtowns (Perhaps this would be better for us to discuss on IRC or something, but timezones...! ) I'll try to focus my reply on the most relevant point (in my mind) to this PR, though there's certainly a lot more to talk about as well:\r\n\r\n>>Getting back to this proposal: what do you think the downside is of adopting this change, and not processing unrequested transactions? If it breaks a lot of existing software, [...]\r\n\r\n>I don't think it solves the problem -- INV/GETDATA needs to be able to feed us ~7 tx/s to us just to keep up with block capacity and RBF, and if we receive 7 tx's that take 1s to validate each second, we already can't keep up.\r\n\r\nPutting aside whether the approach I described could be made to work, mustn't it be the case that the universe of solutions we can come up with gets bigger if we allow our software the freedom to not process unrequested transactions, versus arguing that we must?  I think if we have a solution that doesn't require this behavior, then of course that's great; I'm not sure that we do, and this seems like a useful step forward for increasing the search space of possible solutions.\r\n\r\nI just want to establish what the arguments are against doing this:\r\n1.  it definitely can't help us \r\n   [while I'm skeptical that we know this now, since I don't have a concrete solution in mind, I agree this is possibly a valid reason!]\r\n2.  it might work, but since we don't know if it would, we should hold off for now \r\n   [this seems like a perfectly reasonable argument against to me, though I don't share this point of view]\r\n3.  there are potentially material downsides to the network from us deploying this, whether or not this benefits us \r\n    [I have no idea whether this is true, but I sort of assume it isn't and/or shouldn't be true, though if it were, it would be helpful to know so that we abandon this line of thinking]\r\n4.  something else I'm missing?\r\n\r\nI think if there's significant uncertainty around (3) then that would make either line of thinking on (1) or (2) more potent, in terms of weighing the risks versus the benefits.  But if no one thinks (3) is a material concern, it just seems to me like giving us more degrees of freedom on this is better than not.  \r\n\r\nI also think that since this doesn't solve anything by itself, that it's fine to wait until we have a concrete proposal that builds on this, if people are concerned that it might both be unnecessary and detrimental to the network.  But my own view is that this is strictly increasing the set of solutions we might be able to deploy and hence is useful preparatory work, so in the absence of objections I'd prefer to see this change: I think it's more likely that someone will someday pick this problem up to solve if we've laid this groundwork, and I worry that without it other solutions will be inadequate.\r\n\r\nThat said, if you or anyone else plans to work on this problem more fully in the near future, that's a pretty good reason to hold off on this PR until more analysis is done and we have a better understanding of what a solution might entail.",
      "created_at" : "2021-02-19T18:25:02Z",
      "html_url" : "https://github.com/bitcoin/bitcoin/pull/21224#issuecomment-782254440",
      "id" : 782254440,
      "issue_url" : "https://api.github.com/repos/bitcoin/bitcoin/issues/21224",
      "node_id" : "MDEyOklzc3VlQ29tbWVudDc4MjI1NDQ0MA==",
      "performed_via_github_app" : null,
      "updated_at" : "2021-02-19T18:25:02Z",
      "url" : "https://api.github.com/repos/bitcoin/bitcoin/issues/comments/782254440",
      "user" : {
         "avatar_url" : "https://avatars.githubusercontent.com/u/7463573?v=4",
         "events_url" : "https://api.github.com/users/sdaftuar/events{/privacy}",
         "followers_url" : "https://api.github.com/users/sdaftuar/followers",
         "following_url" : "https://api.github.com/users/sdaftuar/following{/other_user}",
         "gists_url" : "https://api.github.com/users/sdaftuar/gists{/gist_id}",
         "gravatar_id" : "",
         "html_url" : "https://github.com/sdaftuar",
         "id" : 7463573,
         "login" : "sdaftuar",
         "node_id" : "MDQ6VXNlcjc0NjM1NzM=",
         "organizations_url" : "https://api.github.com/users/sdaftuar/orgs",
         "received_events_url" : "https://api.github.com/users/sdaftuar/received_events",
         "repos_url" : "https://api.github.com/users/sdaftuar/repos",
         "site_admin" : false,
         "starred_url" : "https://api.github.com/users/sdaftuar/starred{/owner}{/repo}",
         "subscriptions_url" : "https://api.github.com/users/sdaftuar/subscriptions",
         "type" : "User",
         "url" : "https://api.github.com/users/sdaftuar"
      }
   },
   {
      "author_association" : "CONTRIBUTOR",
      "body" : "@sdaftuar Maybe we should at least move to the mailing list at least?\r\n\r\nI think as long as `PF_RELAY` bypasses the INV/GETDATA requirement, forbidding TX's that we haven't asked for via GETDATA is fine on the current network (provided that doesn't make tests slower) -- I might think the only value is in deduping tx relay, but that's still valuable enough on the live network that everyone should be doing it.\r\n\r\nI'm more thinking that once we figure out some sane/implementable rate-limiting strategy, that some more careful thought will mean we can apply it at the \"received a TX\" point instead, or generalise it to \"received any sort of message\"; which is more general but would also allow INV handling to be less complicated. I guess that's a variant on (2) -- yes, it's likely that some approach of this nature might work, but I think once we figure one out that does we can adapt it to be better/more general in a way that won't need this patch; and until that point this change doesn't really help much in other ways?\r\n\r\n(On the other hand, I thought the previous idea of ignoring TX's during IBD made sense because we've already said via FEEFILTER that we don't want TX's yet, but I took that as more of dont-waste-time thing than DoS prevention per se)",
      "created_at" : "2021-02-20T04:22:07Z",
      "html_url" : "https://github.com/bitcoin/bitcoin/pull/21224#issuecomment-782557211",
      "id" : 782557211,
      "issue_url" : "https://api.github.com/repos/bitcoin/bitcoin/issues/21224",
      "node_id" : "MDEyOklzc3VlQ29tbWVudDc4MjU1NzIxMQ==",
      "performed_via_github_app" : null,
      "updated_at" : "2021-02-20T04:22:07Z",
      "url" : "https://api.github.com/repos/bitcoin/bitcoin/issues/comments/782557211",
      "user" : {
         "avatar_url" : "https://avatars.githubusercontent.com/u/127186?v=4",
         "events_url" : "https://api.github.com/users/ajtowns/events{/privacy}",
         "followers_url" : "https://api.github.com/users/ajtowns/followers",
         "following_url" : "https://api.github.com/users/ajtowns/following{/other_user}",
         "gists_url" : "https://api.github.com/users/ajtowns/gists{/gist_id}",
         "gravatar_id" : "",
         "html_url" : "https://github.com/ajtowns",
         "id" : 127186,
         "login" : "ajtowns",
         "node_id" : "MDQ6VXNlcjEyNzE4Ng==",
         "organizations_url" : "https://api.github.com/users/ajtowns/orgs",
         "received_events_url" : "https://api.github.com/users/ajtowns/received_events",
         "repos_url" : "https://api.github.com/users/ajtowns/repos",
         "site_admin" : false,
         "starred_url" : "https://api.github.com/users/ajtowns/starred{/owner}{/repo}",
         "subscriptions_url" : "https://api.github.com/users/ajtowns/subscriptions",
         "type" : "User",
         "url" : "https://api.github.com/users/ajtowns"
      }
   },
   {
      "author_association" : "MEMBER",
      "body" : "I don't have anything specific to add to the excellent points made by @sdaftuar and @ajtowns. I share aj's scepticism that this is a beneficial change. I think this is the crucial point:\r\n\r\n> I don't think it solves the problem -- INV/GETDATA needs to be able to feed us ~7 tx/s to us just to keep up with block capacity and RBF, and if we receive 7 tx's that take 1s to validate each second, we already can't keep up.\r\n> \r\n> I just don't see the INV/GETDATA pairing as adding any useful information beyond de-duping across peers; if what we want to do is rate-limit TX messages we receive from a peer IMO we should literally do that -- and we can do it when we get up to the TX message by pausing processing for that peer, there's no need to complicate it by adding it into the INV handling.\r\n\r\nThere are a couple of other very interesting ideas which could be split off from this PR for discussion in IRC or separate issues:\r\n\r\n1. How should orphan reprocessing be handled? We currently reprocess orphan transactions in the context of the peer that provided the parent (but punishing the peer that provided the orphan transaction if it turns out to be consensus-invalid). Would it instead make more sense to reprocess the orphan transaction in the context of the peer that provided the orphan transaction or in a global context?\r\n2. Should we prioritize/rate-limit traffic from peers based on prior behaviour, and if so how? Should it be specialized based on the traffic type (eg txs), or should it apply generally to all message types? My reading is that there's general agreement that we should do something, but it needs careful consideration. The next step might be for someone to come up with a design/proof-of-concept that could be reviewed and criticized.\r\n\r\nI'll add these as discussion topics for next week's p2p irc meeting.\r\n\r\n> On the other hand, I thought the previous idea of ignoring TX's during IBD made sense because we've already said via FEEFILTER that we don't want TX's yet, but I took that as more of dont-waste-time thing than DoS prevention per se\r\n\r\nI also agree with this. The narrower approach of ignoring `tx`s during IBD seems like an obvious win.",
      "created_at" : "2021-02-20T10:25:50Z",
      "html_url" : "https://github.com/bitcoin/bitcoin/pull/21224#issuecomment-782601983",
      "id" : 782601983,
      "issue_url" : "https://api.github.com/repos/bitcoin/bitcoin/issues/21224",
      "node_id" : "MDEyOklzc3VlQ29tbWVudDc4MjYwMTk4Mw==",
      "performed_via_github_app" : null,
      "updated_at" : "2021-02-20T10:25:50Z",
      "url" : "https://api.github.com/repos/bitcoin/bitcoin/issues/comments/782601983",
      "user" : {
         "avatar_url" : "https://avatars.githubusercontent.com/u/1063656?v=4",
         "events_url" : "https://api.github.com/users/jnewbery/events{/privacy}",
         "followers_url" : "https://api.github.com/users/jnewbery/followers",
         "following_url" : "https://api.github.com/users/jnewbery/following{/other_user}",
         "gists_url" : "https://api.github.com/users/jnewbery/gists{/gist_id}",
         "gravatar_id" : "",
         "html_url" : "https://github.com/jnewbery",
         "id" : 1063656,
         "login" : "jnewbery",
         "node_id" : "MDQ6VXNlcjEwNjM2NTY=",
         "organizations_url" : "https://api.github.com/users/jnewbery/orgs",
         "received_events_url" : "https://api.github.com/users/jnewbery/received_events",
         "repos_url" : "https://api.github.com/users/jnewbery/repos",
         "site_admin" : false,
         "starred_url" : "https://api.github.com/users/jnewbery/starred{/owner}{/repo}",
         "subscriptions_url" : "https://api.github.com/users/jnewbery/subscriptions",
         "type" : "User",
         "url" : "https://api.github.com/users/jnewbery"
      }
   },
   {
      "author_association" : "MEMBER",
      "body" : "@ajtowns, Thanks for all the great points. While working on this, I became skeptical of naive\r\ntime-based approach for rate-limiting as much from drop-on-the-floor abundant `TX` messages for the \r\nfollowing reasons :\r\n\r\n* We have a heterogeneity of hosts deployed on the network. If we rely on hardcoded time-bounds, \r\ntransaction X might be classified as expensive-to-validate by node A while being filtered out by\r\nnode B. Of course, we could come with some smart heuristics per-grades of hosts, but it's more\r\nlikely to clogg for nothing low-grades ones, damaging their compact blocks processing\r\n* A high throughput of expensive-to-validate txn is a lawful behavior from your peers. Think about\r\na big LN node suddently closing 10% of its ~100 channels, with an average of 5 pending HTLCs, it's\r\n60 txns to propagate through the network. This peer might burst its validation bandwidth, but those \r\ngood feerate txn are the best candidate for block inclusion. Shunning away from this peer, and you\r\nmight again degrade your compact block processing\r\n\r\nRather than _node-dependent_ checks I would rather favor _message-centric_ checks. As an ulterior\r\nwork building on this one I was thinking about some `StaticTxAnalyse()` runnning in ATMP and\r\nyelling a score weighted by feerate back to `net_processing.cpp`. Such tx static analyzer would\r\nscope number of UTXO spent, size of transaction, number of sigops or other low-cost checks. \r\nFrom such score you can compresse peers announcement buckets. Doesn't work if peers can send you\r\nunrequested transaction message.\r\n\r\nThat said, even before to discuss the best set of DoS mitigations (no silver-bullet!), it would\r\nbetter to refine the problem. A peer's transactions traffic might be okay under regular \r\ntransaction throughput, but gauged as DoSy _relatively_ to another known, better feerate traffic\r\nunder some heavy workload. IMHO, two bounds of the problem might be drawn up while mitigating DoS.\r\nAvoid block-processing fallout of the low-grade peer subset. Favor high-feerate, valid transaction\r\ntraffic.\r\n\r\n--\r\n\r\n> Hmm, actually I don't think even a per-peer-orphan limit would help enough? If you're attack is just increasing latency, then you could connect 20 peers to your victim, have each propose 5 orphan txs that will all take 1s to fail to validate and all depend on a single valid tx; then relay the valid tx. On the next run, you'll take take 20s to process the first of each of those orphans; the run after that you'll take 20s to process the next set, etc -- but your honest peer will also only be getting one message processed in each of these sets of 20s too....\r\n\r\nI think we have a fundemental issue with orphans processing. _Orphanness_ source isn't strictly\r\nassignable. I.e, Mallory might provoke mempool conflicts between Alice and Bob, sending Y to Alice, \r\nY' to Bob. From then, Mallory can send a set of Y childrens to Alice, she will accept them and\r\nrelay forward to Bob. Alice will be the one swalloing the DoS penalty. And now you have a vector\r\nof tx-relay jamming...\r\n\r\nHonestly, I hope once we get package relay we can deprecate and remove orphan processing. Good \r\npropagation of a package should be the responsibility of the issuer (or set of them if you have\r\nnon-interactive chain of transactions).\r\n\r\n--\r\n\r\n> I'm more thinking that once we figure out some sane/implementable rate-limiting strategy, that some more careful thought will mean we can apply it at the \"received a TX\" point instead, or generalise it to \"received any sort of message\"; which is more general but would also allow INV handling to be less complicated.\r\n\r\nI don't know about the generalized approach. Not all messages are safety-critical (\"is globally \r\ndropping unsolicited pongs harmless ? what about severing tx-relay for few hours ?\"), some of\r\nthem are hard to fake, but also propagation of high-feerate transactions is essential to keep\r\nthe chain moving forward. IMO, like we're doing for connection types, it might be better to have\r\ndifferent strategies.\r\n\r\n--\r\n\r\n> I think the fundamental problem here is that we will have lots of cases where a peer can be getting us to use our CPU and it will not be possible for us to tell if the peer is adversarial or not. That's basically the issue with transactions that fail policy checks but pass consensus checks: we can try to be more careful than we currently are about maybe classifying some of those policy checks (such as now-old soft forks, like DERSIG) as consensus rather than policy, but at the end of the day there will always be cases that we aren't sure about. For instance, when we loosen a policy requirement, then older software will think that newer software is relaying invalid stuff, and it's hard for a single node to tell that the looser policy is the one that maybe all its peers are using. (And obviously in general we should be tolerant of different policies on the network, whether due to user modifications, different software, etc.)\r\n\r\nI would rather pass away w.r.t to a smarter typing of our script failures. First, it's really\r\nfootgunish when we have to classify a failed transaction in our reject filter (cf. discussion\r\naround BIP329 implementation). And secondly, as you pointed  it well it's conflicting with policies\r\ntolerance, such you'll always have some exploitation margin around policies disjunction. The enemy\r\nknows the system :/\r\n\r\n(Marking the PR as a draft until further IRC/mail discussions)",
      "created_at" : "2021-02-20T18:05:18Z",
      "html_url" : "https://github.com/bitcoin/bitcoin/pull/21224#issuecomment-782724922",
      "id" : 782724922,
      "issue_url" : "https://api.github.com/repos/bitcoin/bitcoin/issues/21224",
      "node_id" : "MDEyOklzc3VlQ29tbWVudDc4MjcyNDkyMg==",
      "performed_via_github_app" : null,
      "updated_at" : "2021-02-20T18:05:18Z",
      "url" : "https://api.github.com/repos/bitcoin/bitcoin/issues/comments/782724922",
      "user" : {
         "avatar_url" : "https://avatars.githubusercontent.com/u/23310655?v=4",
         "events_url" : "https://api.github.com/users/ariard/events{/privacy}",
         "followers_url" : "https://api.github.com/users/ariard/followers",
         "following_url" : "https://api.github.com/users/ariard/following{/other_user}",
         "gists_url" : "https://api.github.com/users/ariard/gists{/gist_id}",
         "gravatar_id" : "",
         "html_url" : "https://github.com/ariard",
         "id" : 23310655,
         "login" : "ariard",
         "node_id" : "MDQ6VXNlcjIzMzEwNjU1",
         "organizations_url" : "https://api.github.com/users/ariard/orgs",
         "received_events_url" : "https://api.github.com/users/ariard/received_events",
         "repos_url" : "https://api.github.com/users/ariard/repos",
         "site_admin" : false,
         "starred_url" : "https://api.github.com/users/ariard/starred{/owner}{/repo}",
         "subscriptions_url" : "https://api.github.com/users/ariard/subscriptions",
         "type" : "User",
         "url" : "https://api.github.com/users/ariard"
      }
   },
   {
      "_links" : {
         "html" : {
            "href" : "https://github.com/bitcoin/bitcoin/pull/21224#discussion_r582024593"
         },
         "pull_request" : {
            "href" : "https://api.github.com/repos/bitcoin/bitcoin/pulls/21224"
         },
         "self" : {
            "href" : "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/582024593"
         }
      },
      "author_association" : "CONTRIBUTOR",
      "body" : "I'm just trying to understand the boost multi-index data structure, and want to do due diligence whenever I see a `while` loop: This will iterate only as many times as we have actively requested a given TX, correct? So the most number of iterations possible is the total number of peers we have? Because the TX hash is specified, so it's not like we run through the entire data structure.",
      "commit_id" : "63103daf80352fa48b671a66b624a15d0c00d2f7",
      "created_at" : "2021-02-24T14:52:33Z",
      "diff_hunk" : "@@ -678,6 +678,18 @@ class TxRequestTracker::Impl {\n         if (it != m_index.get<ByPeer>().end()) MakeCompleted(m_index.project<ByTxHash>(it));\n     }\n \n+    bool ExpectedTx(NodeId peer, const uint256& txhash)\n+    {\n+        auto it_hash = m_index.get<ByTxHash>().lower_bound(ByTxHashView{txhash, State::REQUESTED, 0});\n+        // We need to traverse all the REQUESTED/COMPLETED announcements to verify we effectively\n+        // requested the txhash from this peer.\n+        while (it_hash != m_index.get<ByTxHash>().end() && it_hash->m_txhash == txhash) {\n+            if (it_hash->m_peer == peer) return true;\n+            ++it_hash;\n+        }",
      "html_url" : "https://github.com/bitcoin/bitcoin/pull/21224#discussion_r582024593",
      "id" : 582024593,
      "line" : 689,
      "node_id" : "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4MjAyNDU5Mw==",
      "original_commit_id" : "63103daf80352fa48b671a66b624a15d0c00d2f7",
      "original_line" : 689,
      "original_position" : 12,
      "original_start_line" : 686,
      "path" : "src/txrequest.cpp",
      "position" : 12,
      "pull_request_review_id" : 597565451,
      "pull_request_url" : "https://api.github.com/repos/bitcoin/bitcoin/pulls/21224",
      "side" : "RIGHT",
      "start_line" : 686,
      "start_side" : "RIGHT",
      "updated_at" : "2021-02-24T14:58:33Z",
      "url" : "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/582024593",
      "user" : {
         "avatar_url" : "https://avatars.githubusercontent.com/u/2084648?v=4",
         "events_url" : "https://api.github.com/users/pinheadmz/events{/privacy}",
         "followers_url" : "https://api.github.com/users/pinheadmz/followers",
         "following_url" : "https://api.github.com/users/pinheadmz/following{/other_user}",
         "gists_url" : "https://api.github.com/users/pinheadmz/gists{/gist_id}",
         "gravatar_id" : "",
         "html_url" : "https://github.com/pinheadmz",
         "id" : 2084648,
         "login" : "pinheadmz",
         "node_id" : "MDQ6VXNlcjIwODQ2NDg=",
         "organizations_url" : "https://api.github.com/users/pinheadmz/orgs",
         "received_events_url" : "https://api.github.com/users/pinheadmz/received_events",
         "repos_url" : "https://api.github.com/users/pinheadmz/repos",
         "site_admin" : false,
         "starred_url" : "https://api.github.com/users/pinheadmz/starred{/owner}{/repo}",
         "subscriptions_url" : "https://api.github.com/users/pinheadmz/subscriptions",
         "type" : "User",
         "url" : "https://api.github.com/users/pinheadmz"
      }
   },
   {
      "author_association" : "CONTRIBUTOR",
      "body" : "<!--cf906140f33d8803c4a75a2196329ecb-->\nð This pull request conflicts with the target branch and [needs rebase](https://github.com/bitcoin/bitcoin/blob/master/CONTRIBUTING.md#rebasing-changes).\n\n<sub>Want to unsubscribe from rebase notifications on this pull request? Just convert this pull request to a \"draft\".</sub>",
      "created_at" : "2021-12-13T23:18:45Z",
      "html_url" : "https://github.com/bitcoin/bitcoin/pull/21224#issuecomment-993000406",
      "id" : 993000406,
      "issue_url" : "https://api.github.com/repos/bitcoin/bitcoin/issues/21224",
      "node_id" : "IC_kwDOABII5847L_vW",
      "performed_via_github_app" : null,
      "reactions" : {
         "+1" : 0,
         "-1" : 0,
         "confused" : 0,
         "eyes" : 0,
         "heart" : 0,
         "hooray" : 0,
         "laugh" : 0,
         "rocket" : 0,
         "total_count" : 0,
         "url" : "https://api.github.com/repos/bitcoin/bitcoin/issues/comments/993000406/reactions"
      },
      "updated_at" : "2021-12-13T23:18:45Z",
      "url" : "https://api.github.com/repos/bitcoin/bitcoin/issues/comments/993000406",
      "user" : {
         "avatar_url" : "https://avatars.githubusercontent.com/u/39886733?v=4",
         "events_url" : "https://api.github.com/users/DrahtBot/events{/privacy}",
         "followers_url" : "https://api.github.com/users/DrahtBot/followers",
         "following_url" : "https://api.github.com/users/DrahtBot/following{/other_user}",
         "gists_url" : "https://api.github.com/users/DrahtBot/gists{/gist_id}",
         "gravatar_id" : "",
         "html_url" : "https://github.com/DrahtBot",
         "id" : 39886733,
         "login" : "DrahtBot",
         "node_id" : "MDQ6VXNlcjM5ODg2NzMz",
         "organizations_url" : "https://api.github.com/users/DrahtBot/orgs",
         "received_events_url" : "https://api.github.com/users/DrahtBot/received_events",
         "repos_url" : "https://api.github.com/users/DrahtBot/repos",
         "site_admin" : false,
         "starred_url" : "https://api.github.com/users/DrahtBot/starred{/owner}{/repo}",
         "subscriptions_url" : "https://api.github.com/users/DrahtBot/subscriptions",
         "type" : "User",
         "url" : "https://api.github.com/users/DrahtBot"
      }
   }
]
